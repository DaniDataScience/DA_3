---
title: "DA HW3 Summary_report"
author: "Gyebnar Daniel"
date: '2022 02 15 '
output: html_document
---

# Assessment of fast growing firms

## Abstract
FastInvest aims to invest EUR 100,000 in firms identified as fast growing. 
Based on their investment policy they purchase firms that are expected to grow by at lest 50% annually, and hold them for 5 years before realizing the prot from selling.

The goal of this project was to identify firms with such potential using the tools of prediction and classification by using logit, lasso, and random forest models. This report details summarise key results and the modeling process in a high level.

## Executive summary

The task was to select fast growing firms, defined as firms growing at least 50% annually.

For this, I buil Logit, LASSO, and Random Forest models. The data was used for this analysis was collected, maintained, and cleaned by Bisnode. I defined fast growing as having at least a 50% annual growth in the 2012-2014 period.

I defined the loss function with False Negatives being 7 times worse as False Positives, based on the opportunity of realizing 50% annual growth on the initial investment of EUR 100,000 for 5 years of holding period leading to 700,000 in profits.


I built 3 models, Logit, LASSO and Random Forest. The X3 model gave teh best prediction results, followed by the random forest. I selected the classification threshold as xxx with an algorythm, which is very close to the formula of 1/(1+7)=0.125. 

Based on this threshold, I got the following results:
```{r}

```

The accuracy of the model was 83% meaning that is classified 83% of the firms to the good category. From the actual not fast growing firms the model predicted 92% correctly, while from the actual fast growing ones it predicted 30% right. 

As the number of firms that are actually fast growing is much lower only 16% of our data, so it is more difficult to predict them then to predict non fast growing companies.

From the firms the model predicted to be fast growing 43% actually turned out to be fast growing. T

The expected loss is xxx, meaning the expected loss per purchase is xxx. This is lower to the xxx of the worst model, meaning that building a better model yielded in xxx savings.

#### Data cleaning process
The data originally had c.290,000 observations on c.46,000 firms from 2005 to 2016, with 48 variables. In the end of data cleaning I ended up with 14,139 observations (each a firm from 2012) and 116 variables.

#### Selecting observations
I filtered the data for the 2010-2015 period, and included firms with a revenue between EUR 1,000 - 1,000,000 to capture SMEs. I excluded observations of firms that are no longer active, and also restricted the firms to 2012 (base year of prediction), and below CAGR of 3000 (to exclude potential mistakes and unprecedented events of growth that are not likely to be repeated in case of a purchase)

#### Adding new variables
I added explanatory variables, such as log sales, previous CAGRs (2010-'11 and 2011-'12), foreign management variable and CEO age. I included ratios for key financials by dividing all balance sheet elements with total assets, and similarly with profit and loss statement elements. 

### Droping variables, Imputation  corrections
I dropped variables that had too many missing values (above 15,000 in raw dataset). I used winsorization to correct values for certain items that are not possible based on domain knowledge, and flagged them.

In the end, I had the following distributions:
```{r, echo=FALSE, out.width="50%", fig.cap="Histogram of CAGR", fig.align = 'center'}
library(kableExtra)
knitr::include_graphics("https://raw.githubusercontent.com/DaniDataScience/DA_3/main/DA3/Assignment_3/plot_cagr_hist.png")
```

#### Creating models
I grouped variables according to their meaning, and created formulas for the models. THe grouping description is detailed in the Technical Report. 
From these groups, I created the following models:
```{r, echo=F, message=F, warning=F, fig.cap="Models used"}
models <- data.frame(row.names = c("X1 model", "X2 model", "X3 model", "X4 model", "X5 model", "LASSO", "Random Forest"))
models$Variables[1] <- "Log sales + Log sales^2 + Change in Sales + Profit and loss + Industry"
models$Variables[2] <- "X1 + Fixed assets + Equity + Current liabilities (and flags) + Age + Foreign management"
models$Variables[3] <- "Log sales + Log sales^2 + Firm + Engine variables 1 + D1"
models$Variables[4] <- "X3 + Engine variables 2 + Engine variables 3 + HR"
models$Variables[5] <- "X4 + Interactions 1 and 2"
models$Variables[6] <- "X5"
models$Variables[7] <- "sales_mil + d1_sales_mil_log + rawvars + hr + firm + qualityvars"
models %>% 
  kbl() %>% 
  kable_classic(full_width = F, html_font = "Cambria")
```

#### Results with no loss function 
I created a holdout set from the 20% of the data. The total set, the holdout set and the train set as well had a c.11% ratio of fast growing firms. The train data was used for a 5-fold cross validation.

The best model was X3 logit model based on giving the largest AUC result. 

#### Results with loss function
Based on the formula the optimal classification threshold would be 1/(1+7) = 0.125, but also used an algorithm to calculate optimal thresholds. The results can be found below:

```{r, echo=FALSE, out.width="50%", fig.cap="CV thresholds and Expected loss", fig.align = 'center'}
library(kableExtra)
knitr::include_graphics("")

```

#### Model choice and evaluation
Based table expected loss in the table above, the random forest is the best model to use due to lowest expected losses, followed by .... Interestingly, ordering the models based on AUC would yield a different list.

On the holdout set for my chosen model, the Random Forest, the expected loss is 0.39. This is even smaller than what we got for the train data. Below is the confusion table for the model, showing the choices made by the model, in comparison to the actual values. Based on this the model performance is the following:

Accuracy is 82.6%
Sensitivity is 30.5%
Specificity is 92.4%
Correctly predicted positive 43.1% (115/(115+152))

